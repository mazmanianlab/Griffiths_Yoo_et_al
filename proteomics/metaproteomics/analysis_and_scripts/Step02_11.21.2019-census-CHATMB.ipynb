{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import cm\n",
    "from collections import OrderedDict\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import re\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.models\n",
    "import bokeh.palettes\n",
    "import bokeh.plotting\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "bokeh.io.output_notebook\n",
    "\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002644062042236328\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#------------------------------Using Raw Census PLine Output---------------------------------\n",
    "\n",
    "def extract_PLines(census_file):\n",
    "    \"\"\"\n",
    "        Input: Census filled file (extracting intensities from P-Line)\n",
    "        Output: New dataframe containing data from census PLine\n",
    "    \"\"\"\n",
    "    with open(census_file, 'r') as f:\n",
    "        file1 = f.readlines()\n",
    "    with open('dump1.txt', 'w') as g:    \n",
    "        for line1 in file1:\n",
    "            line2 = line1.split('\\t')\n",
    "            if line2[0] == 'PLINE':\n",
    "                g.write(line1)\n",
    "            elif line2[0] == 'P':\n",
    "                g.write(line1)\n",
    "    temp_df1 = pd.read_csv('dump1.txt', sep='\\t', na_values='-').fillna(0)\n",
    "    return temp_df1\n",
    "\n",
    "def extract_Norm_Raw_PLine_Intensities(temp_df_NR1):\n",
    "    \"\"\"\n",
    "        Input: dataframe derived from 'extract_PLines' \n",
    "        Output: new dataframe containing normalized intensities for each protein/sample\n",
    "    \"\"\"\n",
    "    #Import Select Columns to new dataframe then add 1 to every intensity value (to ensure no 0 values)\n",
    "    temp_df_NR2 = temp_df_NR1[['ACCESSION', 'DESCRIPTION', 'NORM_INTENSITY_1', 'NORM_INTENSITY_2', 'NORM_INTENSITY_3', 'NORM_INTENSITY_4', 'NORM_INTENSITY_5', 'NORM_INTENSITY_6', 'NORM_INTENSITY_7', 'NORM_INTENSITY_8', 'NORM_INTENSITY_9', 'NORM_INTENSITY_10', 'NORM_INTENSITY_11', 'NORM_INTENSITY_12', 'NORM_INTENSITY_13', 'NORM_INTENSITY_14', 'NORM_INTENSITY_15', 'NORM_INTENSITY_16', 'NORM_INTENSITY_17']].copy()\n",
    "    temp_df_NR2[['NORM_INTENSITY_1', 'NORM_INTENSITY_2', 'NORM_INTENSITY_3', 'NORM_INTENSITY_4', 'NORM_INTENSITY_5', 'NORM_INTENSITY_6', 'NORM_INTENSITY_7', 'NORM_INTENSITY_8', 'NORM_INTENSITY_9', 'NORM_INTENSITY_10', 'NORM_INTENSITY_11', 'NORM_INTENSITY_12', 'NORM_INTENSITY_13', 'NORM_INTENSITY_14', 'NORM_INTENSITY_15', 'NORM_INTENSITY_16', 'NORM_INTENSITY_17']] += 1\n",
    "    return temp_df_NR2\n",
    "\n",
    "def extract_Raw_PLine_Intensities(temp_df_R1):\n",
    "    \"\"\"\n",
    "        Input: dataframe derived from 'extract_PLines'\n",
    "        Output: new dataframe containing non-normalized intensities for each protein/sample\n",
    "    \"\"\"\n",
    "    #Import Select Columns to new dataframe then add 1 to every intensity value (to ensure no 0 values)\n",
    "    temp_df_R2 = temp_df_R1[['ACCESSION', 'DESCRIPTION', 'INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']].copy()\n",
    "    temp_df_R2[['INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']] += 1\n",
    "    return temp_df_R2\n",
    "\n",
    "def normalize_Raw_PLine_Intensities(temp_df_R3):\n",
    "    \"\"\"\n",
    "        Input: non-normalized intensities dataframe from 'extract_Raw_PLine_Intensities' function\n",
    "        Output: return same dataframe with normalized intensity values; each value in a column is divided by its specific column sum\n",
    "    \"\"\"\n",
    "    temp_allsums1 = temp_df_R3.sum(axis=0, skipna = True)\n",
    "    temp_df_R3['INTENSITY_1'] /= temp_allsums1['INTENSITY_1']\n",
    "    temp_df_R3['INTENSITY_2'] /= temp_allsums1['INTENSITY_2']\n",
    "    temp_df_R3['INTENSITY_3'] /= temp_allsums1['INTENSITY_3']\n",
    "    temp_df_R3['INTENSITY_4'] /= temp_allsums1['INTENSITY_4']\n",
    "    temp_df_R3['INTENSITY_5'] /= temp_allsums1['INTENSITY_5']\n",
    "    temp_df_R3['INTENSITY_6'] /= temp_allsums1['INTENSITY_6']\n",
    "    temp_df_R3['INTENSITY_7'] /= temp_allsums1['INTENSITY_7']\n",
    "    temp_df_R3['INTENSITY_8'] /= temp_allsums1['INTENSITY_8']\n",
    "    temp_df_R3['INTENSITY_9'] /= temp_allsums1['INTENSITY_9']\n",
    "    temp_df_R3['INTENSITY_10'] /= temp_allsums1['INTENSITY_10']\n",
    "    temp_df_R3['INTENSITY_11'] /= temp_allsums1['INTENSITY_11']\n",
    "    temp_df_R3['INTENSITY_12'] /= temp_allsums1['INTENSITY_12']\n",
    "    temp_df_R3['INTENSITY_13'] /= temp_allsums1['INTENSITY_13']\n",
    "    temp_df_R3['INTENSITY_14'] /= temp_allsums1['INTENSITY_14']\n",
    "    temp_df_R3['INTENSITY_15'] /= temp_allsums1['INTENSITY_15']\n",
    "    temp_df_R3['INTENSITY_16'] /= temp_allsums1['INTENSITY_16']\n",
    "    temp_df_R3['INTENSITY_17'] /= temp_allsums1['INTENSITY_17']\n",
    "    return temp_df_R3\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#----------------------------Clustering Proteins - Census SLine-----------------------------\n",
    "\n",
    "\n",
    "def extract_SLine_from_Census(census_file):\n",
    "    \"\"\"\n",
    "        Input: Census 'filled' file\n",
    "        Output: text dump and returns a peptide dataframe\n",
    "    \"\"\"\n",
    "    with open(census_file, 'r') as f:\n",
    "        file1 = f.readlines()\n",
    "    with open('dump1.txt', 'w') as g:\n",
    "        for line1 in file1:\n",
    "            if (line1[0] == 'SLINE') or (line1[0] == 'S'):\n",
    "                g.write(line1)\n",
    "    df1 = pd.read_csv('dump1.txt', sep='\\t', header=0).fillna('0')\n",
    "    return df1\n",
    "\n",
    "def select_Census_Columns_Peptides(temp_df1):\n",
    "    \"\"\"\n",
    "        Input: raw census peptide (SLine) dataframe [Reliant on 'extract_SLine_from_Census']\n",
    "        Output: new cleaned up dataframe containing peptides (no duplicates) and corresponding intensities for each run\n",
    "    \"\"\"\n",
    "    #Note that columns below represent Peptide sequence and Intensity Columns -- CHANGE AS NECESSARY\n",
    "    temp_df2 = temp_df1[['SEQUENCE_1','SEQUENCE_2','SEQUENCE_3','SEQUENCE_4','SEQUENCE_5','SEQUENCE_6','SEQUENCE_7','SEQUENCE_8','SEQUENCE_9','SEQUENCE_10','SEQUENCE_11','SEQUENCE_12','SEQUENCE_13','SEQUENCE_14','SEQUENCE_15','SEQUENCE_16','SEQUENCE_17','INTENSITY_1','INTENSITY_2','INTENSITY_3','INTENSITY_4','INTENSITY_5','INTENSITY_6','INTENSITY_7','INTENSITY_8','INTENSITY_9','INTENSITY_10','INTENSITY_11','INTENSITY_12','INTENSITY_13','INTENSITY_14','INTENSITY_15','INTENSITY_16','INTENSITY_17']].copy()\n",
    "    \n",
    "    #Find sequence from dataframe SEQUENCE_# values\n",
    "    temp_SEQUENCE = []\n",
    "    for row1 in temp_df2.itertuples():\n",
    "        temp_list1 = []\n",
    "        temp_list1.append(row1.SEQUENCE_1)\n",
    "        temp_list1.append(row1.SEQUENCE_2)\n",
    "        temp_list1.append(row1.SEQUENCE_3)\n",
    "        temp_list1.append(row1.SEQUENCE_4)\n",
    "        temp_list1.append(row1.SEQUENCE_5)\n",
    "        temp_list1.append(row1.SEQUENCE_6)\n",
    "        temp_list1.append(row1.SEQUENCE_7)\n",
    "        temp_list1.append(row1.SEQUENCE_8)\n",
    "        temp_list1.append(row1.SEQUENCE_9)\n",
    "        temp_list1.append(row1.SEQUENCE_10)\n",
    "        temp_list1.append(row1.SEQUENCE_11)\n",
    "        temp_list1.append(row1.SEQUENCE_12)\n",
    "        temp_list1.append(row1.SEQUENCE_13)\n",
    "        temp_list1.append(row1.SEQUENCE_14)\n",
    "        temp_list1.append(row1.SEQUENCE_15)  \n",
    "        temp_list1.append(row1.SEQUENCE_16)\n",
    "        temp_list1.append(row1.SEQUENCE_17)\n",
    "        item2 = 0\n",
    "        for item1 in temp_list1:\n",
    "            if item1 != '0':\n",
    "                item2 = item1\n",
    "                break\n",
    "        temp_SEQUENCE.append(item2)\n",
    "    temp_df2['SEQUENCE'] = pd.Series(temp_SEQUENCE, index=temp_df2.index)\n",
    "    temp_df2 = temp_df2[['SEQUENCE','INTENSITY_1','INTENSITY_2','INTENSITY_3','INTENSITY_4','INTENSITY_5','INTENSITY_6','INTENSITY_7','INTENSITY_8','INTENSITY_9','INTENSITY_10','INTENSITY_11','INTENSITY_12','INTENSITY_13','INTENSITY_14','INTENSITY_15','INTENSITY_16','INTENSITY_17']].copy()\n",
    "    temp_df2['INTENSITY_1'] = pd.to_numeric(temp_df2['INTENSITY_1'], errors='coerce')\n",
    "    temp_df2['INTENSITY_2'] = pd.to_numeric(temp_df2['INTENSITY_2'], errors='coerce')\n",
    "    temp_df2['INTENSITY_3'] = pd.to_numeric(temp_df2['INTENSITY_3'], errors='coerce')\n",
    "    temp_df2['INTENSITY_4'] = pd.to_numeric(temp_df2['INTENSITY_4'], errors='coerce')\n",
    "    temp_df2['INTENSITY_5'] = pd.to_numeric(temp_df2['INTENSITY_5'], errors='coerce')\n",
    "    temp_df2['INTENSITY_6'] = pd.to_numeric(temp_df2['INTENSITY_6'], errors='coerce')\n",
    "    temp_df2['INTENSITY_7'] = pd.to_numeric(temp_df2['INTENSITY_7'], errors='coerce')\n",
    "    temp_df2['INTENSITY_8'] = pd.to_numeric(temp_df2['INTENSITY_8'], errors='coerce')\n",
    "    temp_df2['INTENSITY_9'] = pd.to_numeric(temp_df2['INTENSITY_9'], errors='coerce')\n",
    "    temp_df2['INTENSITY_10'] = pd.to_numeric(temp_df2['INTENSITY_10'], errors='coerce')\n",
    "    temp_df2['INTENSITY_11'] = pd.to_numeric(temp_df2['INTENSITY_11'], errors='coerce')\n",
    "    temp_df2['INTENSITY_12'] = pd.to_numeric(temp_df2['INTENSITY_12'], errors='coerce')\n",
    "    temp_df2['INTENSITY_13'] = pd.to_numeric(temp_df2['INTENSITY_13'], errors='coerce')\n",
    "    temp_df2['INTENSITY_14'] = pd.to_numeric(temp_df2['INTENSITY_14'], errors='coerce')\n",
    "    temp_df2['INTENSITY_15'] = pd.to_numeric(temp_df2['INTENSITY_15'], errors='coerce')\n",
    "    temp_df2['INTENSITY_16'] = pd.to_numeric(temp_df2['INTENSITY_16'], errors='coerce')\n",
    "    temp_df2['INTENSITY_17'] = pd.to_numeric(temp_df2['INTENSITY_17'], errors='coerce')\n",
    "    temp_df2 = temp_df2.fillna(0)\n",
    "    \n",
    "    #Remove rows containing duplicate information (i.e. intensities); Duplicate peptides with different intensities are kept\n",
    "    temp_df3 = temp_df2.drop_duplicates(keep='first', inplace=False)\n",
    "    \n",
    "    #Reformatting Peptides to remove C- and N-terminal cleavage sites and diff-mod sites\n",
    "    temp_col1 = temp_df3['SEQUENCE']\n",
    "    temp_col2 = []\n",
    "    for temp_position1,temp_item1 in enumerate(temp_col1):\n",
    "        temp_col2.append(temp_item1[2:-2].replace('(15.994915)', ''))\n",
    "    temp_df3 = temp_df3.reset_index()\n",
    "    del temp_df3['index']\n",
    "    temp_df4 = temp_df3.copy()\n",
    "    temp_df4['PEPTIDE'] = pd.Series(temp_col2, index=temp_df4.index)\n",
    "    \n",
    "    #Create new cleaned up dataframe\n",
    "    temp_df5 = temp_df4[['PEPTIDE', 'INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']]\n",
    "    temp_df6 = temp_df5.drop_duplicates(keep='first', inplace=False)\n",
    "    temp_df6 = temp_df6.reset_index()\n",
    "    del temp_df6['index']\n",
    "    \n",
    "    #Sort new cleaned up dataframe and sum intensities columns for identical peptides\n",
    "    temp_df6.sort_values('PEPTIDE')\n",
    "    temp_df7 = temp_df6.groupby(['PEPTIDE']).sum()\n",
    "    return temp_df7\n",
    "\n",
    "def extract_PLine_from_Census(census_file):\n",
    "    \"\"\"\n",
    "        Input: census \"filled\" file\n",
    "        Output: text dump and returns a dictionary connecting peptides (key) and protein list (value) redundant proteins included\n",
    "    \"\"\"\n",
    "    #open census file\n",
    "    with open(census_file, 'r') as f:\n",
    "        file1 = f.readlines()\n",
    "    #generate new file containing protein PLines\n",
    "    with open('dump2.txt', 'w') as g:\n",
    "        temp_pep_list1 = []\n",
    "        for line1 in file1:\n",
    "            line2 = line1.split('\\t')\n",
    "            if (line2[0] == 'P'):\n",
    "                temp_pep_list2 = list(set(temp_pep_list1))\n",
    "                for item1 in temp_pep_list2:\n",
    "                    g.write('\\t' + item1 + '\\n')\n",
    "                temp_pep_list1 = []\n",
    "                g.write(line2[1] + '\\t' + line2[2] + '\\n')\n",
    "            elif (line2[0] == 'S'):\n",
    "                item02 = ''\n",
    "                for item01 in line2:\n",
    "                    if '-.' in item01:\n",
    "                        item02 = item01\n",
    "                        break\n",
    "                temp_pep_list1.append(item02[2:-2].replace('(15.994915)', ''))\n",
    "        temp_pep_list2 = list(set(temp_pep_list1))\n",
    "        for item1 in temp_pep_list2:\n",
    "            g.write('\\t' + item1 + '\\n')\n",
    "            temp_pep_list1 = []\n",
    "    \n",
    "    #open newly created protein PLine file\n",
    "    with open('dump2.txt', 'r') as f:\n",
    "        file2 = f.readlines()\n",
    "    \n",
    "    #create peptide(key)-protein(value) dictionary; proteins are assembled in list\n",
    "    Pep_to_Prot_Dict1 = {}\n",
    "    temp_prot1 = \"\"\n",
    "    for line1 in file2:\n",
    "        line2 = line1.split('\\t')\n",
    "        if line2[0] != '':\n",
    "            temp_prot1 = line2[0]\n",
    "        else:\n",
    "            if line2[1].replace('\\n', '') in Pep_to_Prot_Dict1.keys():\n",
    "                Pep_to_Prot_Dict1[line2[1].replace('\\n', '')].append(temp_prot1)\n",
    "            else:\n",
    "                Pep_to_Prot_Dict1[line2[1].replace('\\n', '')] = []\n",
    "                Pep_to_Prot_Dict1[line2[1].replace('\\n', '')].append(temp_prot1)\n",
    "    return Pep_to_Prot_Dict1\n",
    "\n",
    "def map_Clusters_to_Proteins(cluster_file):\n",
    "    \"\"\"\n",
    "        Input: CDHIT cluster file\n",
    "        Output: dictionary connecting Protein (keys) to Cluster number (values)\n",
    "    \"\"\"\n",
    "    #create dictionary connecting protein (keys) and cluster number (values)\n",
    "    with open(cluster_file, 'r') as f:\n",
    "        file1 = f.readlines() \n",
    "    temp_cluster_num = \"\"\n",
    "    Protein_to_Cluster_Dict1 = {}\n",
    "    for line1 in file1:\n",
    "        line2 = line1.split('\\t')        \n",
    "        if '>' in line2[0]:\n",
    "            temp_cluster_num = line2[0].replace('>', '').replace('\\n', '').replace('Cluster ', '')\n",
    "        else:\n",
    "            line3 = line2[1].split(' ')\n",
    "            line4 = line3[1].replace('>', '').replace('...', '')\n",
    "            if '|' in line4:\n",
    "                line4b = line4.split('|')\n",
    "                line4 = line4b[1]\n",
    "            if line4[0:7] != 'Reverse':\n",
    "                Protein_to_Cluster_Dict1[line4] = temp_cluster_num\n",
    "    return Protein_to_Cluster_Dict1\n",
    "\n",
    "def map_Clusternum_to_Peptides(prot_to_clust_dict2, pep_to_prot_dict2):\n",
    "    \"\"\"\n",
    "        Input: 2 dictionaries: protein-cluster and peptide-protein\n",
    "        Output: dictionary linking peptides(key) to cluster number (value)\n",
    "    \"\"\"\n",
    "    pep_to_cluster_dict1 = {}\n",
    "    for item1 in pep_to_prot_dict2.keys():\n",
    "        temp_protlist1 = pep_to_prot_dict2[item1]\n",
    "        temp_cluster_list1 = []\n",
    "        for item2 in temp_protlist1:\n",
    "            try:\n",
    "                temp_cluster_list1.append(prot_to_clust_dict2[item2])\n",
    "            except:\n",
    "                pass\n",
    "        pep_to_cluster_dict1[item1] = list(set(temp_cluster_list1))\n",
    "    return pep_to_cluster_dict1\n",
    "\n",
    "def clusternum_to_PepDataframe(pep_to_clust_dict, uniquepep_dataframe):\n",
    "    \"\"\"\n",
    "        Input: Peptide-to-Cluster dictionary generated from 'map_Clusternum_to_Peptides' function and cleaned-up Peptide-Intensity dataframe from 'select_Census_Columns_Peptides' function\n",
    "        Output: New peptide-intensity dataframe with integrated cluster number column; Note that peptides with no cluster belong to contaminant or reverse proteins\n",
    "    \"\"\"\n",
    "    temp_df1 = uniquepep_dataframe.reset_index()\n",
    "    temp_list1 = list(temp_df1['PEPTIDE'])\n",
    "    temp_clusterlist1 = []\n",
    "    for item1 in temp_list1:\n",
    "        temp_clusterlist1.append(pep_to_clust_dict[item1])\n",
    "    temp_df1['CLUSTER'] = pd.Series(temp_clusterlist1, index=temp_df1.index)\n",
    "    temp_df2 = temp_df1[['PEPTIDE', 'CLUSTER', 'INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']]\n",
    "    return temp_df2\n",
    "\n",
    "def extract_Intensity_Master(census_file1, cluster_file1):\n",
    "    \"\"\"\n",
    "        Input: census 'filled' file and CDHIT cluster file\n",
    "        Output: Dataframe with Peptide, Cluster, and Intensity Columns\n",
    "    \"\"\"\n",
    "    temp_df1A = extract_SLine_from_Census(census_file1)\n",
    "    temp_df2A = select_Census_Columns_Peptides(temp_df1A)\n",
    "    temp_dict1A = extract_PLine_from_Census(census_file1)\n",
    "    temp_dict2A = map_Clusters_to_Peptides(cluster_file1)\n",
    "    temp_dict3A = map_Clusternum_to_Peptides(temp_dict2A, temp_dict1A)\n",
    "    temp_df3A = clusternum_to_PepDataframe(temp_dict3A, temp_df2A)\n",
    "    return temp_df3A\n",
    "\n",
    "def create_Cluster_Intensity_Table(complete_dataframe):\n",
    "    \"\"\"\n",
    "        Input: complete master dataframe from 'extract_Intensity_Master'\n",
    "        Output: new dataframe with intensities summed by cluster\n",
    "    \"\"\"\n",
    "    #Collect only peptides that map to 1 cluster\n",
    "    temp_df1A = complete_dataframe.loc[complete_dataframe['CLUSTER'].str.len() == 1]\n",
    "    #Create new column (ClusterID) then sum intensities belonging to the same cluster\n",
    "    temp_list1 = temp_df1A['CLUSTER']\n",
    "    temp_list2 = []\n",
    "    for item1 in temp_list1:\n",
    "        temp_list2.append(item1[0])\n",
    "    temp_df1A['ClusterID'] = pd.Series(temp_list2, index=temp_df1A.index)\n",
    "    temp_df1A = temp_df1A[['PEPTIDE', 'CLUSTER', 'ClusterID', 'INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']]\n",
    "    cluster_df1 = temp_df1A.groupby(['ClusterID']).sum().copy()    \n",
    "    #Add 1 to each value to ensure no 0 values (necessary for Log transform and subsequent hypothesis testing)\n",
    "    cluster_df1[['INTENSITY_1', 'INTENSITY_2', 'INTENSITY_3', 'INTENSITY_4', 'INTENSITY_5', 'INTENSITY_6', 'INTENSITY_7', 'INTENSITY_8', 'INTENSITY_9', 'INTENSITY_10', 'INTENSITY_11', 'INTENSITY_12', 'INTENSITY_13', 'INTENSITY_14', 'INTENSITY_15', 'INTENSITY_16', 'INTENSITY_17']] += 0\n",
    "    return cluster_df1\n",
    "\n",
    "def normalize_Cluster_Intensity_Table(complete_dataframe_2):\n",
    "    \"\"\"\n",
    "        Input: Cluster-Intensity dataframe\n",
    "        Output: New Cluster-Intensity dataframe with each entry normalized by dividing by respective column sum (note that intensities from peptides mapping to multiple clusters tossed)\n",
    "    \"\"\"\n",
    "    temp_allsums1 = complete_dataframe_2.sum(axis=0, skipna = True)\n",
    "    complete_dataframe_2['INTENSITY_1'] /= temp_allsums1['INTENSITY_1']\n",
    "    complete_dataframe_2['INTENSITY_2'] /= temp_allsums1['INTENSITY_2']\n",
    "    complete_dataframe_2['INTENSITY_3'] /= temp_allsums1['INTENSITY_3']\n",
    "    complete_dataframe_2['INTENSITY_4'] /= temp_allsums1['INTENSITY_4']\n",
    "    complete_dataframe_2['INTENSITY_5'] /= temp_allsums1['INTENSITY_5']\n",
    "    complete_dataframe_2['INTENSITY_6'] /= temp_allsums1['INTENSITY_6']\n",
    "    complete_dataframe_2['INTENSITY_7'] /= temp_allsums1['INTENSITY_7']\n",
    "    complete_dataframe_2['INTENSITY_8'] /= temp_allsums1['INTENSITY_8']\n",
    "    complete_dataframe_2['INTENSITY_9'] /= temp_allsums1['INTENSITY_9']\n",
    "    complete_dataframe_2['INTENSITY_10'] /= temp_allsums1['INTENSITY_10']\n",
    "    complete_dataframe_2['INTENSITY_11'] /= temp_allsums1['INTENSITY_11']\n",
    "    complete_dataframe_2['INTENSITY_12'] /= temp_allsums1['INTENSITY_12']\n",
    "    complete_dataframe_2['INTENSITY_13'] /= temp_allsums1['INTENSITY_13']\n",
    "    complete_dataframe_2['INTENSITY_14'] /= temp_allsums1['INTENSITY_14']\n",
    "    complete_dataframe_2['INTENSITY_15'] /= temp_allsums1['INTENSITY_15']\n",
    "    complete_dataframe_2['INTENSITY_16'] /= temp_allsums1['INTENSITY_16']\n",
    "    complete_dataframe_2['INTENSITY_17'] /= temp_allsums1['INTENSITY_17']\n",
    "    return complete_dataframe_2\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#----------------------------Formatting and Annotating R-output-----------------------------\n",
    "\n",
    "def annotate_R_abridged(cdhit_file, abridged_R_file):\n",
    "    \"\"\"\n",
    "        Input: CDHIT cluster file and abridged R outputfile containing p-adj values\n",
    "        Output: newly annotated dataframe\n",
    "    \"\"\"\n",
    "    with open(cdhit_file, 'r') as f:\n",
    "        file1 = f.readlines()\n",
    "    cluster_representative_dict1 = {}\n",
    "    for line1 in file1:\n",
    "        line2 = line1.split('\\t')\n",
    "        if line1[0] == '>':\n",
    "            temp_clusternum = line2[0].replace('>Cluster ', '').replace('\\n', '')\n",
    "        elif '... *' in line2[1]:\n",
    "            line3 = line2[1].split(' ')\n",
    "            line4 = line3[1].replace('>', '').replace('...', '')\n",
    "            cluster_representative_dict1[temp_clusternum] = line4\n",
    "            temp_clusternum = \"\"\n",
    "    with open(abridged_R_file, 'r') as g:\n",
    "        file2 = g.readlines()\n",
    "    cluster_rep_list = []\n",
    "    for line1 in file2:\n",
    "        line2 = line1.split(',')\n",
    "        if line2[1] in cluster_representative_dict1.keys():\n",
    "            cluster_rep_list.append(cluster_representative_dict1[line2[1]])\n",
    "    temp_dfC1 = pd.read_csv(abridged_R_file) \n",
    "    temp_dfC1['cluster_representative'] = pd.Series(cluster_rep_list, index=temp_dfC1.index)\n",
    "    return temp_dfC1\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.121624231338501\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#---------------Prepare Tables for R-analysis of Census PLine Data-------------------\n",
    "\n",
    "#convert census_filled into file with only PLines\n",
    "censusP_df1 = extract_PLines('Census/census-chat-mouse-20318_filled_modded.txt')\n",
    "\n",
    "#extract specific columns from converted census file\n",
    "Norm_PLine_df1 = extract_Norm_Raw_PLine_Intensities(censusP_df1)\n",
    "Non_Norm_PLine_df1 = extract_Raw_PLine_Intensities(censusP_df1)\n",
    "\n",
    "#normalize columns in Non_Norm extracted file\n",
    "Non_Norm_PLine_df1 = normalize_Raw_PLine_Intensities(Non_Norm_PLine_df1)\n",
    "\n",
    "#output files to csv ready for R-analysis\n",
    "Norm_PLine_df1.to_csv('PLine-ChatMouse-NormIntensityTable1.txt', index=False)\n",
    "Non_Norm_PLine_df1.to_csv('PLine-ChatMouse-NonNormIntensityTable1.txt', index=False)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PTB/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.32936120033264\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Only need to run once for each CDHIT-# analysis (take ~1.5 hours)\n",
    "#temp_df1A = extract_SLine_from_Census('Census/census-chat-microbiome-20338_modded.txt')\n",
    "\n",
    "#--------Prepare Tables for R-analysis of Census SLine Data (Clustering)-------------\n",
    "\n",
    "#CHAT-Microbiome, CDHIT 65 (Run once per analysis)\n",
    "temp_df2A = select_Census_Columns_Peptides(temp_df1A)\n",
    "temp_dict1A = extract_PLine_from_Census('Census/census-chat-microbiome-20338_modded.txt')\n",
    "temp_dict2A = map_Clusters_to_Proteins('CDHIT/cdhitout-chat-65.clstr')\n",
    "temp_dict3A = map_Clusternum_to_Peptides(temp_dict2A, temp_dict1A)\n",
    "temp_chatMB_65 = clusternum_to_PepDataframe(temp_dict3A, temp_df2A)\n",
    "chatMB_65 = create_Cluster_Intensity_Table(temp_chatMB_65)\n",
    "#chatMB_65 = normalize_Cluster_Intensity_Table(chatMB_65)\n",
    "\n",
    "#CHAT-Microbiome, CDHIT 75 (Run once per analysis)\n",
    "temp_df2A = select_Census_Columns_Peptides(temp_df1A)\n",
    "temp_dict1A = extract_PLine_from_Census('Census/census-chat-microbiome-20338_modded.txt')\n",
    "temp_dict2A = map_Clusters_to_Proteins('CDHIT/cdhitout-chat-75.clstr')\n",
    "temp_dict3A = map_Clusternum_to_Peptides(temp_dict2A, temp_dict1A)\n",
    "temp_chatMB_75 = clusternum_to_PepDataframe(temp_dict3A, temp_df2A)\n",
    "chatMB_75 = create_Cluster_Intensity_Table(temp_chatMB_75)\n",
    "#chatMB_75 = normalize_Cluster_Intensity_Table(chatMB_75)\n",
    "\n",
    "#CHAT-Microbiome, CDHIT 85 (Run once per analysis)\n",
    "temp_df2A = select_Census_Columns_Peptides(temp_df1A)\n",
    "temp_dict1A = extract_PLine_from_Census('Census/census-chat-microbiome-20338_modded.txt')\n",
    "temp_dict2A = map_Clusters_to_Proteins('CDHIT/cdhitout-chat-85.clstr')\n",
    "temp_dict3A = map_Clusternum_to_Peptides(temp_dict2A, temp_dict1A)\n",
    "temp_chatMB_85 = clusternum_to_PepDataframe(temp_dict3A, temp_df2A)\n",
    "chatMB_85 = create_Cluster_Intensity_Table(temp_chatMB_85)\n",
    "#chatMB_85 = normalize_Cluster_Intensity_Table(chatMB_85)\n",
    "\n",
    "#CHAT-Microbiome, CDHIT 95 (Run once per analysis)\n",
    "temp_df2A = select_Census_Columns_Peptides(temp_df1A)\n",
    "temp_dict1A = extract_PLine_from_Census('Census/census-chat-microbiome-20338_modded.txt')\n",
    "temp_dict2A = map_Clusters_to_Proteins('CDHIT/cdhitout-chat-95.clstr')\n",
    "temp_dict3A = map_Clusternum_to_Peptides(temp_dict2A, temp_dict1A)\n",
    "temp_chatMB_95 = clusternum_to_PepDataframe(temp_dict3A, temp_df2A)\n",
    "chatMB_95 = create_Cluster_Intensity_Table(temp_chatMB_95)\n",
    "#chatMB_95 = normalize_Cluster_Intensity_Table(chatMB_95)\n",
    "\n",
    "#Output to txt/csv file\n",
    "chatMB_65.to_csv('SLine-ChatMB-NormIntensityTable-65.txt')\n",
    "chatMB_75.to_csv('SLine-ChatMB-NormIntensityTable-75.txt')\n",
    "chatMB_85.to_csv('SLine-ChatMB-NormIntensityTable-85.txt')\n",
    "chatMB_95.to_csv('SLine-ChatMB-NormIntensityTable-95.txt')\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENSITY_1</th>\n",
       "      <th>INTENSITY_2</th>\n",
       "      <th>INTENSITY_3</th>\n",
       "      <th>INTENSITY_4</th>\n",
       "      <th>INTENSITY_5</th>\n",
       "      <th>INTENSITY_6</th>\n",
       "      <th>INTENSITY_7</th>\n",
       "      <th>INTENSITY_8</th>\n",
       "      <th>INTENSITY_9</th>\n",
       "      <th>INTENSITY_10</th>\n",
       "      <th>INTENSITY_11</th>\n",
       "      <th>INTENSITY_12</th>\n",
       "      <th>INTENSITY_13</th>\n",
       "      <th>INTENSITY_14</th>\n",
       "      <th>INTENSITY_15</th>\n",
       "      <th>INTENSITY_16</th>\n",
       "      <th>INTENSITY_17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClusterID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [INTENSITY_1, INTENSITY_2, INTENSITY_3, INTENSITY_4, INTENSITY_5, INTENSITY_6, INTENSITY_7, INTENSITY_8, INTENSITY_9, INTENSITY_10, INTENSITY_11, INTENSITY_12, INTENSITY_13, INTENSITY_14, INTENSITY_15, INTENSITY_16, INTENSITY_17]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatMB_65[chatMB_65.eq(0).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7230849266052246\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#---------------Add Protein Labels to R-analysis of Census SLine Data----------------\n",
    "\n",
    "chatmouse_65_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-65.clstr', 'Census_analysis/SLine-ChatMB-Census-Log2Transform-65.csv')\n",
    "chatmouse_65_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_65_t_R1.to_csv('SLine-ChatMB-Census-Log2Transform-65-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_75_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-75.clstr', 'Census_analysis/SLine-ChatMB-Census-Log2Transform-75.csv')\n",
    "chatmouse_75_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_75_t_R1.to_csv('SLine-ChatMB-Census-Log2Transform-75-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_85_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-85.clstr', 'Census_analysis/SLine-ChatMB-Census-Log2Transform-85.csv')\n",
    "chatmouse_85_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_85_t_R1.to_csv('SLine-ChatMB-Census-Log2Transform-85-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_95_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-95.clstr', 'Census_analysis/SLine-ChatMB-Census-Log2Transform-95.csv')\n",
    "chatmouse_95_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_95_t_R1.to_csv('SLine-ChatMB-Census-Log2Transform-95-annotated.txt', index=False)\n",
    "\n",
    "\n",
    "#----------\n",
    "\n",
    "chatmouse_65_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-65.clstr', 'Census_analysis/SLine-ChatMB-Census-cuberootTransform-65.csv')\n",
    "chatmouse_65_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_65_t_R1.to_csv('SLine-ChatMB-Census-cuberootTransform-65-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_75_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-75.clstr', 'Census_analysis/SLine-ChatMB-Census-cuberootTransform-75.csv')\n",
    "chatmouse_75_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_75_t_R1.to_csv('SLine-ChatMB-Census-cuberootTransform-75-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_85_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-85.clstr', 'Census_analysis/SLine-ChatMB-Census-cuberootTransform-85.csv')\n",
    "chatmouse_85_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_85_t_R1.to_csv('SLine-ChatMB-Census-cuberootTransform-85-annotated.txt', index=False)\n",
    "\n",
    "chatmouse_95_t_R1 = annotate_R_abridged('CDHIT/cdhitout-chat-95.clstr', 'Census_analysis/SLine-ChatMB-Census-cuberootTransform-95.csv')\n",
    "chatmouse_95_t_R1.drop(columns='Unnamed: 0', inplace=True)\n",
    "chatmouse_95_t_R1.to_csv('SLine-ChatMB-Census-cuberootTransform-95-annotated.txt', index=False)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PEPTIDE</th>\n",
       "      <th>CLUSTER</th>\n",
       "      <th>INTENSITY_1</th>\n",
       "      <th>INTENSITY_2</th>\n",
       "      <th>INTENSITY_3</th>\n",
       "      <th>INTENSITY_4</th>\n",
       "      <th>INTENSITY_5</th>\n",
       "      <th>INTENSITY_6</th>\n",
       "      <th>INTENSITY_7</th>\n",
       "      <th>INTENSITY_8</th>\n",
       "      <th>INTENSITY_9</th>\n",
       "      <th>INTENSITY_10</th>\n",
       "      <th>INTENSITY_11</th>\n",
       "      <th>INTENSITY_12</th>\n",
       "      <th>INTENSITY_13</th>\n",
       "      <th>INTENSITY_14</th>\n",
       "      <th>INTENSITY_15</th>\n",
       "      <th>INTENSITY_16</th>\n",
       "      <th>INTENSITY_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PEPTIDE, CLUSTER, INTENSITY_1, INTENSITY_2, INTENSITY_3, INTENSITY_4, INTENSITY_5, INTENSITY_6, INTENSITY_7, INTENSITY_8, INTENSITY_9, INTENSITY_10, INTENSITY_11, INTENSITY_12, INTENSITY_13, INTENSITY_14, INTENSITY_15, INTENSITY_16, INTENSITY_17]\n",
       "Index: []"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_chatMB_65.loc[temp_chatMB_65['CLUSTER'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatMB_65_mod = chatMB_65[(chatMB_65 != 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INTENSITY_1</th>\n",
       "      <th>INTENSITY_2</th>\n",
       "      <th>INTENSITY_3</th>\n",
       "      <th>INTENSITY_4</th>\n",
       "      <th>INTENSITY_5</th>\n",
       "      <th>INTENSITY_6</th>\n",
       "      <th>INTENSITY_7</th>\n",
       "      <th>INTENSITY_8</th>\n",
       "      <th>INTENSITY_9</th>\n",
       "      <th>INTENSITY_10</th>\n",
       "      <th>INTENSITY_11</th>\n",
       "      <th>INTENSITY_12</th>\n",
       "      <th>INTENSITY_13</th>\n",
       "      <th>INTENSITY_14</th>\n",
       "      <th>INTENSITY_15</th>\n",
       "      <th>INTENSITY_16</th>\n",
       "      <th>INTENSITY_17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClusterID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>36271295.0</td>\n",
       "      <td>41177119.0</td>\n",
       "      <td>48722983.0</td>\n",
       "      <td>3.713498e+08</td>\n",
       "      <td>18707019.0</td>\n",
       "      <td>84630621.0</td>\n",
       "      <td>1.246717e+08</td>\n",
       "      <td>108314651.0</td>\n",
       "      <td>2.967816e+08</td>\n",
       "      <td>134661393.0</td>\n",
       "      <td>49488823.0</td>\n",
       "      <td>7.482563e+08</td>\n",
       "      <td>1.213554e+09</td>\n",
       "      <td>1.855307e+08</td>\n",
       "      <td>3.562673e+08</td>\n",
       "      <td>5.241841e+08</td>\n",
       "      <td>79272822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>85014707.0</td>\n",
       "      <td>30809303.0</td>\n",
       "      <td>58384466.0</td>\n",
       "      <td>6.273672e+08</td>\n",
       "      <td>136200422.0</td>\n",
       "      <td>34470792.0</td>\n",
       "      <td>5.226969e+08</td>\n",
       "      <td>18315050.0</td>\n",
       "      <td>1.098933e+09</td>\n",
       "      <td>95320282.0</td>\n",
       "      <td>51405654.0</td>\n",
       "      <td>3.469384e+07</td>\n",
       "      <td>4.057677e+08</td>\n",
       "      <td>4.529110e+08</td>\n",
       "      <td>5.426110e+08</td>\n",
       "      <td>5.567733e+08</td>\n",
       "      <td>65742725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>223791247.0</td>\n",
       "      <td>178912079.5</td>\n",
       "      <td>291389232.0</td>\n",
       "      <td>2.010431e+08</td>\n",
       "      <td>183142036.0</td>\n",
       "      <td>250305557.0</td>\n",
       "      <td>4.609302e+08</td>\n",
       "      <td>145601246.0</td>\n",
       "      <td>3.483979e+08</td>\n",
       "      <td>136853862.0</td>\n",
       "      <td>128549721.0</td>\n",
       "      <td>2.853296e+08</td>\n",
       "      <td>2.952686e+08</td>\n",
       "      <td>1.414721e+08</td>\n",
       "      <td>1.319867e+08</td>\n",
       "      <td>1.751864e+08</td>\n",
       "      <td>277092548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>25036878.0</td>\n",
       "      <td>17009530.0</td>\n",
       "      <td>33637517.0</td>\n",
       "      <td>9.215520e+07</td>\n",
       "      <td>52609333.0</td>\n",
       "      <td>46997004.0</td>\n",
       "      <td>4.153678e+07</td>\n",
       "      <td>1361326.0</td>\n",
       "      <td>7.853596e+07</td>\n",
       "      <td>21273020.0</td>\n",
       "      <td>31031195.0</td>\n",
       "      <td>2.125327e+08</td>\n",
       "      <td>1.196071e+07</td>\n",
       "      <td>7.407990e+06</td>\n",
       "      <td>4.126720e+07</td>\n",
       "      <td>8.368831e+07</td>\n",
       "      <td>36682524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>13843780.0</td>\n",
       "      <td>89296192.0</td>\n",
       "      <td>18101583.0</td>\n",
       "      <td>5.198729e+08</td>\n",
       "      <td>34505584.0</td>\n",
       "      <td>62776028.0</td>\n",
       "      <td>6.622167e+07</td>\n",
       "      <td>4544984.0</td>\n",
       "      <td>1.057189e+09</td>\n",
       "      <td>210911205.0</td>\n",
       "      <td>59838685.0</td>\n",
       "      <td>2.024458e+08</td>\n",
       "      <td>3.205426e+07</td>\n",
       "      <td>3.718354e+08</td>\n",
       "      <td>1.135164e+09</td>\n",
       "      <td>1.443453e+09</td>\n",
       "      <td>64133604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>919705367.0</td>\n",
       "      <td>741870053.0</td>\n",
       "      <td>518418418.5</td>\n",
       "      <td>6.153664e+09</td>\n",
       "      <td>828545516.0</td>\n",
       "      <td>667848789.5</td>\n",
       "      <td>3.540808e+09</td>\n",
       "      <td>331462006.0</td>\n",
       "      <td>6.341385e+09</td>\n",
       "      <td>333242206.0</td>\n",
       "      <td>516931975.0</td>\n",
       "      <td>2.247789e+09</td>\n",
       "      <td>3.985327e+09</td>\n",
       "      <td>1.296462e+09</td>\n",
       "      <td>2.876408e+09</td>\n",
       "      <td>8.858046e+09</td>\n",
       "      <td>977764513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>24001426.0</td>\n",
       "      <td>13024396.0</td>\n",
       "      <td>25232577.0</td>\n",
       "      <td>1.948847e+07</td>\n",
       "      <td>34720934.0</td>\n",
       "      <td>35374249.0</td>\n",
       "      <td>7.709741e+07</td>\n",
       "      <td>7632493.0</td>\n",
       "      <td>6.164563e+07</td>\n",
       "      <td>16064269.0</td>\n",
       "      <td>11576254.0</td>\n",
       "      <td>1.983756e+07</td>\n",
       "      <td>6.392284e+07</td>\n",
       "      <td>7.968789e+06</td>\n",
       "      <td>3.707071e+07</td>\n",
       "      <td>9.551162e+07</td>\n",
       "      <td>14148838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>108443383.0</td>\n",
       "      <td>254443139.0</td>\n",
       "      <td>114025200.0</td>\n",
       "      <td>1.976698e+07</td>\n",
       "      <td>78275402.0</td>\n",
       "      <td>196051245.0</td>\n",
       "      <td>1.529614e+08</td>\n",
       "      <td>55559571.0</td>\n",
       "      <td>2.790194e+08</td>\n",
       "      <td>54356141.0</td>\n",
       "      <td>26346451.0</td>\n",
       "      <td>1.280181e+08</td>\n",
       "      <td>9.748841e+07</td>\n",
       "      <td>2.391836e+07</td>\n",
       "      <td>3.478370e+07</td>\n",
       "      <td>3.083833e+07</td>\n",
       "      <td>42456345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>26805538.0</td>\n",
       "      <td>13560197.0</td>\n",
       "      <td>27635602.0</td>\n",
       "      <td>1.224502e+08</td>\n",
       "      <td>13320182.0</td>\n",
       "      <td>51974365.0</td>\n",
       "      <td>1.037991e+08</td>\n",
       "      <td>125210138.0</td>\n",
       "      <td>3.454695e+08</td>\n",
       "      <td>124423900.0</td>\n",
       "      <td>60620904.0</td>\n",
       "      <td>3.403964e+08</td>\n",
       "      <td>3.296250e+08</td>\n",
       "      <td>3.162010e+08</td>\n",
       "      <td>4.053249e+08</td>\n",
       "      <td>5.775937e+08</td>\n",
       "      <td>56648220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>186574219.0</td>\n",
       "      <td>171077639.0</td>\n",
       "      <td>264162076.0</td>\n",
       "      <td>1.388115e+09</td>\n",
       "      <td>301450592.0</td>\n",
       "      <td>488141875.0</td>\n",
       "      <td>1.487473e+09</td>\n",
       "      <td>856083196.0</td>\n",
       "      <td>1.925644e+09</td>\n",
       "      <td>539454168.0</td>\n",
       "      <td>213500809.0</td>\n",
       "      <td>2.338574e+09</td>\n",
       "      <td>5.289019e+09</td>\n",
       "      <td>2.579787e+09</td>\n",
       "      <td>1.724427e+09</td>\n",
       "      <td>2.152687e+09</td>\n",
       "      <td>390287808.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           INTENSITY_1  INTENSITY_2  INTENSITY_3   INTENSITY_4  INTENSITY_5  \\\n",
       "ClusterID                                                                     \n",
       "105         36271295.0   41177119.0   48722983.0  3.713498e+08   18707019.0   \n",
       "1098        85014707.0   30809303.0   58384466.0  6.273672e+08  136200422.0   \n",
       "1109       223791247.0  178912079.5  291389232.0  2.010431e+08  183142036.0   \n",
       "112         25036878.0   17009530.0   33637517.0  9.215520e+07   52609333.0   \n",
       "1122        13843780.0   89296192.0   18101583.0  5.198729e+08   34505584.0   \n",
       "...                ...          ...          ...           ...          ...   \n",
       "865        919705367.0  741870053.0  518418418.5  6.153664e+09  828545516.0   \n",
       "874         24001426.0   13024396.0   25232577.0  1.948847e+07   34720934.0   \n",
       "897        108443383.0  254443139.0  114025200.0  1.976698e+07   78275402.0   \n",
       "93          26805538.0   13560197.0   27635602.0  1.224502e+08   13320182.0   \n",
       "930        186574219.0  171077639.0  264162076.0  1.388115e+09  301450592.0   \n",
       "\n",
       "           INTENSITY_6   INTENSITY_7  INTENSITY_8   INTENSITY_9  INTENSITY_10  \\\n",
       "ClusterID                                                                       \n",
       "105         84630621.0  1.246717e+08  108314651.0  2.967816e+08   134661393.0   \n",
       "1098        34470792.0  5.226969e+08   18315050.0  1.098933e+09    95320282.0   \n",
       "1109       250305557.0  4.609302e+08  145601246.0  3.483979e+08   136853862.0   \n",
       "112         46997004.0  4.153678e+07    1361326.0  7.853596e+07    21273020.0   \n",
       "1122        62776028.0  6.622167e+07    4544984.0  1.057189e+09   210911205.0   \n",
       "...                ...           ...          ...           ...           ...   \n",
       "865        667848789.5  3.540808e+09  331462006.0  6.341385e+09   333242206.0   \n",
       "874         35374249.0  7.709741e+07    7632493.0  6.164563e+07    16064269.0   \n",
       "897        196051245.0  1.529614e+08   55559571.0  2.790194e+08    54356141.0   \n",
       "93          51974365.0  1.037991e+08  125210138.0  3.454695e+08   124423900.0   \n",
       "930        488141875.0  1.487473e+09  856083196.0  1.925644e+09   539454168.0   \n",
       "\n",
       "           INTENSITY_11  INTENSITY_12  INTENSITY_13  INTENSITY_14  \\\n",
       "ClusterID                                                           \n",
       "105          49488823.0  7.482563e+08  1.213554e+09  1.855307e+08   \n",
       "1098         51405654.0  3.469384e+07  4.057677e+08  4.529110e+08   \n",
       "1109        128549721.0  2.853296e+08  2.952686e+08  1.414721e+08   \n",
       "112          31031195.0  2.125327e+08  1.196071e+07  7.407990e+06   \n",
       "1122         59838685.0  2.024458e+08  3.205426e+07  3.718354e+08   \n",
       "...                 ...           ...           ...           ...   \n",
       "865         516931975.0  2.247789e+09  3.985327e+09  1.296462e+09   \n",
       "874          11576254.0  1.983756e+07  6.392284e+07  7.968789e+06   \n",
       "897          26346451.0  1.280181e+08  9.748841e+07  2.391836e+07   \n",
       "93           60620904.0  3.403964e+08  3.296250e+08  3.162010e+08   \n",
       "930         213500809.0  2.338574e+09  5.289019e+09  2.579787e+09   \n",
       "\n",
       "           INTENSITY_15  INTENSITY_16  INTENSITY_17  \n",
       "ClusterID                                            \n",
       "105        3.562673e+08  5.241841e+08    79272822.0  \n",
       "1098       5.426110e+08  5.567733e+08    65742725.0  \n",
       "1109       1.319867e+08  1.751864e+08   277092548.0  \n",
       "112        4.126720e+07  8.368831e+07    36682524.0  \n",
       "1122       1.135164e+09  1.443453e+09    64133604.0  \n",
       "...                 ...           ...           ...  \n",
       "865        2.876408e+09  8.858046e+09   977764513.0  \n",
       "874        3.707071e+07  9.551162e+07    14148838.0  \n",
       "897        3.478370e+07  3.083833e+07    42456345.0  \n",
       "93         4.053249e+08  5.775937e+08    56648220.0  \n",
       "930        1.724427e+09  2.152687e+09   390287808.0  \n",
       "\n",
       "[145 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatMB_65_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatMB_65_contr = chatMB_65.iloc[:,0:9].copy()\n",
    "chatMB_65_treat = chatMB_65.iloc[:,9:17].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatMB_65_contr = chatMB_65_contr[(chatMB_65_contr != 0).any(1)]\n",
    "chatMB_65_treat = chatMB_65_treat[(chatMB_65_treat != 0).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
